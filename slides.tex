\documentclass{beamer}

\usepackage[utf8]{inputenc}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}

\DeclareMathOperator{\Uniform}{Uniform}

\setbeamercolor{block body alerted}{bg=alerted text.fg!10}
\setbeamercolor{block title alerted}{bg=alerted text.fg!20}
\setbeamercolor{block body}{bg=structure!10}
\setbeamercolor{block title}{bg=structure!20}
\setbeamercolor{block body example}{bg=green!10}
\setbeamercolor{block title example}{bg=green!20}
\setbeamertemplate{blocks}[rounded][shadow]

\title{Perturbation-Stable Maximum Cut}
\author{Yuchong Pan}
\institute{UBC Beyond Worst-Case Analysis Reading Group \\ (Based on Tim Roughgarden's Notes for Stanford CS264)}
%\date{\today}
\date{June 30, 2020}

\begin{document}
    \frame{\titlepage}

    \begin{frame}
        \frametitle{{\sc Maximum Cut}}
    
        \begin{block}{Problem ({\sc Maximum Cut})}
            \setlength{\leftmargini}{3.25em}
            \begin{itemize}
                \item[\bf Input:] An undirected graph $G = (V, E)$ with edge weights $w_e > 0$ for each $e \in E$.
                \item[\bf Goal:] A cut $(A, B)$ that maximizes the weight of the {\bf crossing} edges. 
            \end{itemize}
        \end{block}

        \pause

        \begin{itemize}
            \item {\sc Maximum Cut} is a type of $2$-clustering problem (e.g. weights measure dissimilarities).
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{{\sc Maximum Cut} Is $NP$-Hard}
    
        \begin{block}{Problem ({\sc Maximum Cut}, Decision Version)}
            \setlength{\leftmargini}{4em}
            \begin{itemize}
                \item[\bf Input:] An undirected graph $G = (V, E)$ with edge weights $w_e > 0$ for each $e \in E$, and a positive integer $W$.
                \item[\bf Output:] Yes iff.\ there is a set $S \subseteq V$ such that the weight of the {\bf crossing} edges is at least $W$.
            \end{itemize}
        \end{block}

        \pause

        \begin{block}{Problem ({\sc Partition}, Decision Version)}
            \setlength{\leftmargini}{4em}
            \begin{itemize}
                \item[\bf Input:] $(c_1, \ldots, c_n) \in \ZZ^n$.
                \item[\bf Output:] Yes iff.\ there is $I \subseteq [n]$ such that $\sum_{i \in I} c_i = \sum_{i \not \in I} c_i$.
            \end{itemize}
        \end{block}

        \pause

        \begin{block}{Proof Sketch ($\textsc{Partition} \leq_P \textsc{Maximum Cut}$)}
            \begin{itemize}
                \item $G = K_n$.
                \item $w_{ij} = c_i c_j$ for all $i, j \in V, i \neq j$.
                \item $W = \lceil \frac{1}{4} \sum c_i^2 \rceil$.
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{{\sc Maximum Cut} Is $NP$-Hard}
    
        \begin{itemize}
            \item {\sc Minimum Cut} is {\bf not} $NP$-hard and can be solved by the Maximum-Flow Minimum-Cut Theorem. \pause
            \item {\bf Question:} Can't we negate the edge weights, yielding a {\sc Minimum Cut} instance? \pause
            \item No! Polynomial-time algorithms solving {\sc Minimum Cut} require nonnegative edge weights.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Exact Recovery}
    
        \begin{itemize}
            \item {\bf Theme:} To recover the optimal solution in polynomial time in {\bf $\gamma$-perturbation-stable} instances, where $\gamma$ is as small as possible.
        \end{itemize}

        \pause

        \begin{definition}[$\gamma$-Perturbation-Stability]
            For $\gamma \geq 1$, an instance of {\sc Maximum Cut} is {\bf $\gamma$-perturbation-stable} if a cut $(A, B)$ is the {\bf unique} optimal solution to all {\bf $\gamma$-perturbations}, where each original edge weight $w_e$ is replaced with an edge weight $w_e' \in [\frac{1}{\gamma} w_e, w_e]$.
        \end{definition}
    \end{frame}

    \begin{frame}
        \frametitle{LP Relaxation, Take 1}
    
        \begin{itemize}
            \item {\bf Question:} Can we use an LP relaxation similar to the one for {\sc Minimum Cut}, i.e.
            \begin{alignat*}{4}
                && \max \qquad && \sum_{e \in E} w_e x_e \\
                && \text{s.t.} \qquad && x_e & \geq \left|d_u - d_v\right|, & \qquad \forall e = uv & \in E, \\
                && && x_e & \in [0, 1], & \qquad \forall e & \in E, \\
                && && d_v & \in [0, 1], & \qquad \forall v & \in V.
              \end{alignat*}
              \pause
              \vspace{-1em}
              \item No! $x_e = 1$ for each $e \in E$ is a feasible solution and maximizes the objective value. \pause
              \item {\bf Question:} What about $x_e \leq d_u - d_v$ and $x_e \leq d_v - d_u$? \pause
              \item This forces $x_e = 0$, instead of $x_e \leq |d_u - d_v|$.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{LP Relaxation, Take 2}
    
        \begin{itemize}
            \item Let $x_{ij} \in \{ 0, 1 \}$ denote whether or not $i, j$ are on different sides of the cut, for all distinct $i, j \in V$. We denote by $x_{ij}$ and $x_{ji}$ the same variable. \pause
            \item {\bf Intuition:} If $i, j$ are on different sides, and $i, k$ are also on different sides, then $j, k$ must be on the same side. \pause
            \item For any distinct $i, j, k \in V$, at most two of $x_{ij}, x_{ik}, x_{jk}$ are $1$. \pause
            \begin{align*}
                x_{ij} + x_{ik} + x_{jk} \leq 2, && \forall i, j, k \in V \text{ distinct}.
            \end{align*}
            \pause
            \vspace{-1em}
            \item {\bf Intuition:} If $i, j$ are on the same side, and $i, k$ are on the same side, then $j, k$ are on the same side. \pause
            \item For any distinct $i, j, k \in V$, $x_{ij} = x_{ik} = 0$ implies $x_{jk} = 0$. \pause
            \begin{align*}
                x_{jk} \leq x_{ij} + x_{ik}, && \forall i, j, k \in V \text{ distinct}.
            \end{align*}
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{LP Relaxation, Take 2}
    
        \begin{itemize}
            \item Hence we obtain the LP relaxation {\sc (LP-MaxCut)}:
            \begin{alignat*}{4}
                && \max \qquad && \sum_{(i, j) \in E} w_{ij} x_{ij} \\
                && \text{s.t.} \qquad && x_{ij} + x_{ik} + x_{jk} & \leq 2, & \quad \forall i, j, k & \in V \text{ distinct}, \\
                && && x_{jk} & \leq x_{ij} + x_{ik}, & \quad \forall i, j, k & \in V \text{ distinct}, \\
                && && x_{ij} & \in [0, 1], & \quad \forall i, j & \in V \text{ distinct}.
              \end{alignat*}
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Main Theorem}

        \begin{theorem}
            There is a constant $c > 0$ such that in every $(c \log n)$-perturbation-stable instance of {\sc Maximum Cut} with $n$ vertices, {\sc (LP-MaxCut)} solves to integers.
        \end{theorem}

        \pause

        \begin{itemize}
            \item Recall the proofs of exact recovery by LP in $1$-perturbation-stable {\sc Minimum $s$-$t$ Cut} instances and in $4$-perturbation-stable {\sc Minimum Multiway Cut} instances. \pause
            \item In each of the two proofs we design a {\bf randomized rounding algorithm} that outputs a (random) cut such that the probability of an edge being cut is approximately the same as the value of the corresponding decision variable. \pause
            \item {\sc Minimum $s$-$t$ Cut}: $A = \{ v \in V : \hat d_v \leq r \}$ and $B = V \setminus A$, where $r \sim \Uniform(0, 1)$. \pause
            \item {\sc Minimum Multiway Cut}: For each iteration, a group and a threshold are chosen uniformly randomly.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Main Theorem}
    
        \begin{fact}
            LP algorithms (e.g.\ the ellipsoid method) always return an extreme point of the feasible region.
        \end{fact}

        \pause

        \begin{itemize}
            \item Proof omitted here. For the ellipsoid method see e.g.\ CPSC 536S Submodular Optimization. \pause
            \includegraphics[width=0.9\textwidth]{ellipsoid.png}
            \pause
            \item Since all of the extreme points of the feasible region are integral and correspond to a cut, then LP algorithms always solve {\sc (LP-MaxCut)} to an integral optimal solution.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Main Theorem}

        \begin{itemize}
            \item A randomized rounding algorithm implies the exact recovery theorem since: \pause
            \begin{enumerate}
                \item The optimal fractional solution $\mathbf{\hat x}$ can only be better than the optimal integral solution $C^*$; \pause
                \item The randomized rounding algorithm gives a distribution over $s$-$t$ cuts that is as good, on average, as $C^*$; \pause
                \item Hence the distribution must be a point mass on $C^*$.
            \end{enumerate}
            \pause
            \item Formally, we define $\Delta(C)$ to be the total cost of $C$ that exceeds that of $C^*$ and $\Delta(\mathbf{\hat x})$ to be total cost of $C^*$ that exceeds the objective function value of $\mathbf{\hat x}$. \pause
            \item We show that $\EE[\Delta(C)] \leq 0$ by the probability properties of the cut generated by the randomized rounding algorithm. \pause
            \item Since $\Delta(C) \geq 0$ and since the equality holds iff.\ $C$ is an optimal cut, it follows that the randomized rounding algorithm outputs an optimal cut w.p.1.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Randomized Rounding Algorithm}
    
        \begin{lemma}
            Fix an instance of the {\sc Maximum Cut} problem, with $F^*$ the edges in the optimal cut, and $\mathbf{\hat x}$ the optimal solution to {\sc (LP-MaxCut)}. Then there exists a randomized algorithm that generates a random cut $(A, B)$ and a scaling parameter $\sigma > 0$ such that:
            \begin{enumerate}
                \item For every edge $e = ij \not \in F^*$,
                $$ \PP[\text{$e$ cut by $(A, B)$}] \geq \sigma \cdot \frac{\hat x_{ij}}{\alpha}, $$
                where $\alpha = \Theta(\log n)$;
                \item For every edge $e = ij \in F^*$,
                $$ \PP[\text{$e$ not cut by $(A, B)$}] \leq \sigma \cdot (1 - \hat x_{ij}); $$
                \item The rounding algorithm is deterministic iff.\ $\hat x$ is integral.
            \end{enumerate}
        \end{lemma}
    \end{frame}

    \begin{frame}
        \frametitle{Randomized Rounding Algorithm, Roadmap}

        \begin{itemize}
            \item {\bf Exercise:} Show that this lemma implies the main theorem (outlined above, Homework \#4).
        \end{itemize}

        \pause

        \begin{block}{Proposition}
            Fix an instance of {\sc Maximum Cut}, a cut $C$, and a feasible solution $\mathbf{\hat x}$ to {\sc (LP-MaxCut)}. For distinct $i, j \in V$, define
            $$ \hat y_{ij} = \left\{
                \begin{array}{ll}
                    \hat x_{ij}, & \text{if $i, j$ are on the same side of $C$}, \\
                    1 - \hat x_{ij}, & \text{if $i, j$ are on different sides of $C$}.
                \end{array}
            \right. $$
            Then $\mathbf {\hat y}$ satisfies the triangle inequality:
            $$ \hat y_{jk} \leq \hat y_{ij} + \hat y_{ik} $$
            for every $i, j, k \in V$.
        \end{block}

        \pause

        \begin{itemize}
            \item That is, $\mathbf{\hat x}, \mathbf{\hat y}$ are both {\bf pseudometrics} (i.e. metrics except that distinct points may have zero distances).
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Randomized Rounding Algorithm, Roadmap}
    
        \begin{theorem}[Bourgain's Theorem]
            For every $n$-point {\bf pseudometric} space $(X, d)$, there exists a randomized algorithm that generates a random partition $(A, B)$ of $X$ and a scaling parameter $\sigma > 0$ such that, for all distinct $i, j \in X$,
            $$ \PP[\text{$i, j$ on different sides of $(A, B)$}] \in \sigma \cdot \left[\frac{d(i, j)}{\alpha}, d(i, j)\right], $$
            where $\alpha = \Theta(\log n)$.
        \end{theorem}

        \pause

        \begin{itemize}
            \item That is, every $n$-point metric space admits a randomized partitioning algorithm so that the sepration probabilities between pairs of points are {\bf proportional} to the distances, up to a $\Theta(\log n)$ factor. \pause
            \item The $\Theta(\log n)$ approximation factor is the best possible for {\bf arbitrary} pseudometric spaces.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Randomized Rounding Algorithm, Roadmap}
    
        \begin{proof}[Proof (Proposition \& Bourgain's Theorem $\implies$ Lemma)]
            \renewcommand{\qedsymbol}{}
            \begin{itemize}
                \item Fix an instance of {\sc Maximum Cut}. Let $C^*$ denote an optimal cut, cutting the edges $F^*$. \pause
                \item Let $\mathbf{\hat x}$ be an optimal solution to {\sc (LP-MaxCut)}. Define $\mathbf{\hat y}$ as in Proposition (with $C^*$ being the cut). \pause
                \item By Proposition, $\mathbf{\hat y}$ is a pseudometric. \pause
                \item By Bourgain's Theorem, there is a randomized algorithm that outputs a partition $(A, B)$ and $\sigma > 0$ such that
                $$ \PP[\text{$i, j$ on different sides of $(A, B)$}] = \sigma \cdot \left[\frac{\hat y_{ij}}{\alpha}, \hat y_{ij}\right], $$
                where $\alpha = \Theta(\log n)$.
            \end{itemize}
        \end{proof}
    \end{frame}

    \begin{frame}
        \frametitle{Randomized Rounding Algorithm, Roadmap}
    
        \begin{proof}[Proof (Proposition \& Bourgain's Theorem $\implies$ Lemma)]
            \begin{itemize}
                \item By the definition of $\mathbf{\hat y}$,
                \begin{enumerate}
                    \item If $i, j$ are on the same side of $C^*$, then
                    $$ \PP[\text{$i, j$ on different sides of $(A, B)$}] \in \sigma \cdot \left[\frac{\hat x_{ij}}{\alpha}, \hat x_{ij}\right]. $$
                    \item If $i, j$ are on different sides of $C^*$, then
                    $$ \PP[\text{$i, j$ on different sides of $(A, B)$}] \in \sigma \cdot \left[\frac{1 - \hat x_{ij}}{\alpha}, 1 - \hat x_{ij}\right]. $$
                \end{enumerate}
                \pause
                \item (3) follows from the description of the randomized algorithm in Bourgain's Theorem.
            \end{itemize}
        \end{proof}

        \pause

        \begin{itemize}
            \item {\bf Exercise:} Prove Proposition and Bourgain's Theorem (Homework \#4).
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Dimensionality Reduction}

        \begin{definition}[$\alpha$-Embeddings]
            Let $(X, d_X), (Y, d_Y)$ be metric spaces. We say that $\phi : X \to Y$ is an {\bf $\alpha$-embedding} if there exists $r > 0$ such that
            $$ r \cdot d_X(u, v) \leq d_Y(\phi(u), \phi(v)) \leq r \cdot \alpha \cdot d_X(u, v). $$
            for all $u, v \in X$.
        \end{definition}

        \pause

        \begin{itemize}
            \item {\bf Dimensionality reduction} is the process of mapping a high dimensional dataset to a lower dimensional space, while preserving much of the important structure. \pause
            \item For instance, let $X \subseteq \RR^d$ and $Y = \RR^t$ with $t < d$ and $d_X, d_Y$ being the Euclidean distance.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Dimensionality Reduction}
    
        \begin{theorem}[Johnson-Lindenstrauss, 1984]
            Let $x_1, \ldots, x_n \in \RR^d$. Let $\epsilon \in (0, 1)$. Then for some $t = O(\frac{\log(n)}{\epsilon^2})$, there exist $y_1, \ldots, y_n \in \RR^t$ such that
            {\small
                $$
                \begin{array}{rcccll}
                    (1 - \epsilon) \left\lVert x_j \right\rVert & \leq & \left\lVert y_j \right\rVert & \leq & (1 + \epsilon) \left\lVert x_j \right\rVert, & \forall j \in [n], \\
                    (1 - \epsilon) \left\lVert x_j - x_{j'} \right\rVert & \leq & \left\lVert y_j - y_{j'} \right\rVert & \leq & (1 + \epsilon) \left\lVert x_j - x_{j'} \right\rVert, & \forall j, j' \in [n].
                \end{array}
                $$
            }

            \vspace{-1em}
            {\bf Notation:} $\lVert v \rVert = \sqrt{\sum_{i = 1}^n v_i^2}$.
        \end{theorem}

        \pause

        \begin{itemize}
            \item {\bf Remark:} There is a {\bf random linear map} such that for any $x_1, \ldots, x_n$ the above condition holds with probability at least $\frac{1}{2n}$. This linear map is {\bf oblivious}: it does not depend on $x_1, \ldots, x_n$ at all! In fact, the linear map is just a matrix whose entries are independent Gaussians.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Bourgain's Theorem \& {\sc Sparsest Cut}}
    
        \begin{theorem}[Bourgain's Metric Embedding Theorem]
            For any metric space $(V, d)$, there exists an $O(\log n)$-embedding into $\RR^{O(\log^2 n)}$ with the $\ell_1$-norm that is computable with high probability by a randomized polynomial-time algorithm.
        \end{theorem}

        \pause

        \begin{itemize}
            \item This result is the best possible; i.e., there exists a metric that cannot be embedded into $\ell_1$ with distortion less than $\Omega(\log n)$.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Bourgain's Theorem \& {\sc Sparsest Cut}}
    
        \begin{definition}[Cut Metrics]
            A metric $(X, d)$ is a {\bf cut metric} if there exists $S \subseteq X$ such that $d(x, y) = 0$ whenever $x, y \in S$ or $x, y \not \in S$, and $d(x, y) = 1$ otherwise.
        \end{definition}

        \pause

        \begin{lemma}
            A metric $(X, d)$ is an $\ell_1$ metric if and only if it is a nonnegative linear combination of cut metrics.
        \end{lemma}

        \pause

        \begin{itemize}
            \item {\bf Exercise:} Prove Lemma. \pause
            \item Lemma and Bourgain's Metric Embedding Theorem imply Bourgain's Theorem in the proof of the main theorem.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Bourgain's Theorem \& {\sc Sparsest Cut}}

        \begin{block}{Problem ({\sc Sparsest Cut})}
            \setlength{\leftmargini}{3.25em}
            \begin{itemize}
                \item[\bf Input:] An undirected graph $G = (V, E)$ with edge weights $w_e > 0$ for each $e \in E$, and $k$ pairs of vertices $(s_i, t_i)$ each with demand $d_i$.
                \item[\bf Goal:] A set of vertices $S$ that minimizes
                $$ \rho(S) \equiv \frac{\sum_{e \in \delta(S)} c_e}{\sum_{i : |S \cap \{ s_i, t_i \}| = 1} d_i}. $$
            \end{itemize}
        \end{block}

        \pause

        \begin{corollary}
            There is a randomized $O(\log n)$-approximation algorithm for {\sc Sparsest Cut}.
        \end{corollary}
    \end{frame}

    \begin{frame}
        \frametitle{Tree Metric Embedding}
    
        \begin{definition}[Tree Metrics]
            A metric $(X, d)$ is a {\bf tree metric} if there exists a tree $T = (V, E)$ with edge costs $c_e$ for each $e \in E$ such that $d(u, v)$ is the cost of the unique path from $u$ to $v$ in $T$.
        \end{definition}

        \pause

        \begin{theorem}[Fakcharoenphol-Rao-Talwar]
            For any metric $(V, d)$ such that $d(u, v) \geq 1$ for all $u, v \in V$ with $u \neq v$, there exists a randomized, polynomial-time algorithm that produces a tree metric $(V', T), V \subseteq V'$ such that for all $u, v \in V$, we have $d(u, v) \leq T_{uv}$ and $\EE[T_{uv}] \leq O(\log n) d(u, v)$.
        \end{theorem}

        \pause

        \begin{itemize}
            \item The above result is obtained via the method of {\bf hierarchical tree decomposition}.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Semidefinite Programming}
    
        \begin{definition}[Positive Semidefinite Matrices]
            A matrix $\mathbf X \in \RR^{n \times n}$ is {\bf positive semidefinite} (or {\bf psd}), denoted $\mathbf X \succeq 0$, if $\mathbf y^T \mathbf X \mathbf y \geq 0$ for all $y \in \RR^n$.
        \end{definition}

        \begin{fact}
            If $\mathbf X \in \RR^{n \times n}$ is a symmetric matrix, then the following statements are equivalent:
            \begin{enumerate}
                \item $\mathbf X$ is psd;
                \item $\mathbf X$ has nonnegative eigenvalues;
                \item $\mathbf X = \mathbf V^T \mathbf V$ for some $\mathbf V \in \RR^{m \times n}$ where $m \leq n$;
                \item $\mathbf X = \sum_{i = 1}^n \lambda_i \mathbf w_i \mathbf w_i^T$ for some $\lambda_i \geq 0$ and $\mathbf w_i \in \RR^n$ such that $\mathbf w_i^T \mathbf w_i = 1$ and $\mathbf w_i^T \mathbf w_j = 0$ for all $i \neq j$.
            \end{enumerate}
        \end{fact}
    \end{frame}

    \begin{frame}
        \frametitle{Semidefinite Programming}
    
        \begin{definition}[Semidefinite Programming, SDP]
            A {\bf semidefinite program}, or {\bf SDP}, is a mathematical program with real-valued variables, a linear objective function, linear constraints, and a square symmetric matrix of variables constrained to be psd.
        \end{definition}

        \pause

        \begin{itemize}
            \item Below is an example of SDP with variables $x_{ij}$ for $i, j \in [n]$:
            \begin{alignat}{4}
                && \max \text{ or } \min \qquad && \sum_{i, j \in [n]} c_{ij} x_{ij} \label{example:sdp} \\
                && \text{s.t.} \qquad && \sum_{i, j \in [n]} a_{ijk} x_{ij} &= b_k, & \qquad \forall k & \in [n], \nonumber \\
                && && x_{ij} &= x_{ji}, & \qquad \forall i, j & \in [n], \nonumber \\
                && && \mathbf X = \left(x_{ij}\right) & \succeq 0. \nonumber
              \end{alignat}
              \pause
              \vspace{-2em}
              \item SDP can be solved to within an additive error of $\epsilon$ in polynomial time in the size of the input and $\log(\frac{1}{\epsilon})$.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Equivalence of SDP and Vector Programming}
    
        \begin{definition}[Vector Programming]
            A {\bf vector program} is a mathematical program with variables $\mathbf v_i \in \RR^n$, where $n$ is the number of vectors, and an objective function and constraints linear in the inner products of the vectors.
        \end{definition}

        \pause

        \begin{itemize}
            \item Below is an example of vector programming with variables $\mathbf v_i \in \RR^n$ for $i \in [n]$:
            \begin{alignat}{4}
                && \max \text{ or } \min \qquad && \sum_{i, j \in [n]} c_{ij} \left(v_i \cdot v_j\right) \label{example:vector-program} \\
                && \text{s.t.} \qquad && \sum_{i, j \in [n]} a_{ijk} \left(v_i \cdot v_j\right) &= b_k, & \qquad \forall k & \in [n], \nonumber \\
                && && \mathbf v_i &\in \RR^n, & \qquad \forall i & \in [n]. \nonumber
              \end{alignat}
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Equivalence of SDP and Vector Programming}
    
        \begin{theorem}[Equivalence of SDP and Vector Programming]
            The SDP \eqref{example:sdp} and the vector program \eqref{example:vector-program} are equivalent.
        \end{theorem}

        \pause

        \begin{proof}
            \begin{itemize}
                \item ($\Longrightarrow$) Given a solution $\mathbf X$ to \eqref{example:sdp}, compute a matrix $\mathbf V$ such that $\mathbf X = \mathbf V^T \mathbf V$ (within small error), and set $\mathbf v_i$ to be the $i^\text{th}$ column of $\mathbf V$. \pause
                \item Then $x_{ij} = \mathbf v_i^T \mathbf v_j = \mathbf v_i \cdot \mathbf v_j$. \pause
                \item Hence $\mathbf v_i$'s is a feasible solution to \eqref{example:vector-program} with the same value. \pause
                \item ($\Longleftarrow$) Given a solution $\mathbf v_i$'s to \eqref{example:vector-program}, construct a matrix $\mathbf V$ with $i^\text{th}$ column $\mathbf v_i$, and let $\mathbf X = \mathbf V^T \mathbf V$. \pause
                \item Then $\mathbf X$ is symmetric and psd by Fact, with $x_{ij} = \mathbf v_i \cdot \mathbf v_j$. \pause
                \item Hence $\mathbf X$ is a feasible solution to \eqref{example:sdp} with the same value.
            \end{itemize}
        \end{proof}
    \end{frame}

    \begin{frame}
        \frametitle{Quadratic Programming Formulation}
    
        \begin{itemize}
            \item {\bf Recall:} We obtained exact recovery in $\Theta(\log n)$-perturbation-stable instances of {\sc Maximum Cut}. \pause
            \item {\bf Goal:} To get exact recovery for smaller values of $\gamma$ by SDP. \pause
            \item We start with a quadratic programming formulation, which suggests a vector programming relaxation since a vector program contains {\bf inner products} of the vectors. \pause
            \item Let $z_i \in \{ -1, +1 \}$ for each $i \in V$ indicate which side of the cut $i$ belongs to. \pause
            \item Hence, $z_i - z_j$ is $0$ if $i, j$ are on the same side of the cut, and $\pm 2$ otherwise. \pause
            \item Equivalently, $(z_i - z_j)^2$ is $0$ if $i, j$ are on the same side of the cut, and $4$ otherwise.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Quadratic Programming Formulation}
    
        \begin{itemize}
            \item Hence we obtain the {\bf exact} quadratic programming formulation of {\sc Maximum Cut}:
            \begin{alignat*}{4}
                && \max \qquad && \frac{1}{4} \sum_{ij \in E} w_{ij} (z_i - z_j)^2 \\
                && \text{s.t.} \qquad && z_i & \in \{ -1, +1 \}, & \qquad \forall i & \in V.
              \end{alignat*}
              \pause
              \item This quadratic program is {\bf equivalent} to {\sc Maximum Cut}. Hence optimizing this program is $NP$-hard.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Vector Programming Relaxation}
    
        \begin{itemize}
            \item {\bf Recall:} Vector programming and SDP are equivalent and hence can be solved to within small error in polynomial time. \pause
            \item Hence relaxing each $z_i \in \{ -1, 1 \}$ to a {\bf unit vector} $\mathbf v_i \in \RR^n$ and therefore replacing quadratic terms with inner products yield a vector program that is computationally tractable:
            \begin{alignat*}{4}
                && \max \qquad && \frac{1}{4} \sum_{ij \in E} w_{ij} \left\lVert \mathbf v_i - \mathbf v_j \right\rVert^2 \\
                && && \left\lVert \mathbf v_i \right\rVert^2 &= 1, & \qquad \forall i & \in V, \\
                && \text{s.t.} \qquad && \mathbf v_i & \in \RR^n, & \qquad \forall i & \in V.
              \end{alignat*}
              \pause
              \vspace{-1em}
              \item The quadratic terms $\lVert \mathbf v_i - \mathbf v_j \rVert^2$ and $\lVert \mathbf v_i \rVert^2$ expands to sums of inner products $\mathbf v_i \cdot \mathbf v_i - 2 \cdot \mathbf v_i \cdot \mathbf v_j + \mathbf v_j \cdot \mathbf v_j$ and $\mathbf v_i \cdot \mathbf v_i$, respectively. \pause
              \item This vector program is a relaxation of the quadratic program by setting $\mathbf v_i = (z_i, 0, \ldots, 0) \in \RR^n$.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Goemans-Williamson Approximation Algorithm}

        \begin{itemize}
            \item The above vector programming relaxation already leads to the best-known approximation algorithm for {\sc Maximum Cut}:
        \end{itemize}

        \pause

        \begin{theorem}[Goemans-Williamson Approximation Algorithm]
            There is a randomized $.878$-approximation algorithm for {\sc Maximum Cut}.
        \end{theorem}

        \pause

        \begin{theorem}
            Given the unique game conjecture there is no $\alpha$-approximation for {\sc Maximum Cut} with constant $\alpha > .878$ unless $P = NP$.
        \end{theorem}
    \end{frame}

    \begin{frame}
        \frametitle{Vector Programming Relaxation}
    
        \begin{itemize}
            \item For our purposes we want the vector programming relaxation to generalize the LP relaxation. Hence we want the analogs of the following two sets of constraints:
            \begin{align*}
                x_{ij} + x_{ik} + x_{jk} &\leq 2, && \forall i, j, k \in V \text{ distinct}, \\
                x_{jk} &\leq x_{ij} + x_{ik}, && \forall i, j, k \in V \text{ distinct}.
            \end{align*}
            \pause
            \vspace{-1em}
            \item This implies the following two sets of constraints in the form of inner products:
            {\small
                \begin{align*}
                    &\left\lVert \mathbf v_i - \mathbf v_j \right\rVert^2 + \left\lVert \mathbf v_i - \mathbf v_k \right\rVert^2 + \left\lVert \mathbf v_j - \mathbf v_k \right\rVert^2 \leq 8, && \forall i, j, k \in V \text{ distinct}, \\
                    &\left\lVert \mathbf v_j - \mathbf v_k \right\rVert^2 \leq \left\lVert \mathbf v_i - \mathbf v_j \right\rVert^2 + \left\lVert \mathbf v_i - \mathbf v_k \right\rVert^2, && \forall i, j, k \in V \text{ distinct}.
                \end{align*}
            }
            \pause
            \vspace{-1em}
            \item The extended vector program, which we call {\sc (VP-MaxCut)}, is still a relaxation for {\sc Maximum Cut} by setting $\mathbf v_i$ to $\pm e_1$ according to $i$'s side.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Improved Exact Recovery, Roadmap}
    
        \begin{itemize}
            \item Recall the roadmap for the proof of the exact recovery for {\sc Maximum Cut} with $\gamma = \Theta(\log n)$. \pause
            \item We first proved a proposition saying that if $\mathbf{\hat x}$ is a pseudometric, then $\mathbf{\hat y}$ defined below is also a pseudometric:
            $$ \hat y_{ij} = \left\{
                \begin{array}{ll}
                    \hat x_{ij}, & \text{if $i, j$ are on the same side of $C$}, \\
                    1 - \hat x_{ij}, & \text{if $i, j$ are on different sides of $C$}.
                \end{array}
            \right. $$
            \pause
            \item This proposition and Bourgain's Theorem imply the lemma for the existence of randomized rounding algorithm that outputs a (random) cut such that the probability of an edge being cut is approximately the same as the value of the corresponding decision variable. \pause
            \item The lemma then implies the exact recovery theorem by a common pattern used in perturbation-stable {\sc Minimum Cut} and {\sc Minimum Multiway Cut}.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Improved Exact Recovery, Roadmap}
    
        \begin{itemize}
            \item Fix an instance of {\sc Maximum Cut}. Let $\mathbf{\hat v}_i$'s be an optimal solution to the vector program. Let $\hat x_{ij} = \frac{1}{4} \lVert \mathbf v_i - \mathbf v_j \rVert^2$ for any distinct $i, j \in V$. \pause
            \item $\mathbf{\hat x}$ is a metric by the triangle inequality constraints of {\sc (VP-MaxCut)}. \pause
            \item Furthermore, $\hat x_{ij}$ represents the squared Euclidean distances between points in $\RR^k$ for some $k$.
        \end{itemize}

        \pause

        \begin{definition}[$\ell_2^2$ Metrics]
            A metric $(X, d)$ is an {\bf $\ell_2^2$ metric} if it represents squared Euclidean distances between points in $\RR^k$ for some $k$.

            That is, there exists an embedding from $X$ to $\RR^k$ for some $k$ such that $d(\cdot, \cdot)$'s are squared Euclidean distances between points in $\RR^k$.
        \end{definition}
    \end{frame}

    \begin{frame}
        \frametitle{Improved Exact Recovery, Roadmap}

        \begin{itemize}
            \item Not every metric is an $\ell_2^2$ metric, e.g. the discrete metric. \pause
            \item The $\Theta(\log n)$ approximation in Bourgain's Theorem is the best possible for {\bf arbitrary} pseudometric spaces. \pause
            \item {\bf Question:} Can the approximation factor $\alpha$ be improved for the more restricted $\ell_2^2$ metrics?
        \end{itemize}

        \pause
    
        \begin{theorem}[Arora, Lee, and Naor, 2005]
            For every $n$-point $\ell_2^2$ metric space $(X, d)$, there exists a randomized algorithm that generates a random partition $(A, B)$ of $X$ and a scaling parameter $\sigma > 0$ such that, for all distinct $i, j \in X$,
            $$ \PP[\text{$i, j$ on different sides of $(A, B)$}] \in \sigma \cdot \left[\frac{d(i, j)}{\alpha}, d(i, j)\right], $$
            where $\alpha = O(\sqrt{\log n} \log \log n)$.
        \end{theorem}
    \end{frame}

    \begin{frame}
        \frametitle{Improved Exact Recovery, Roadmap}
    
        \begin{itemize}
            \item Arora, Lee, and Naor's Theorem gives the lower bound for the probability of an edge being cut. Similarly, we need an upper bound for the probability of an edge not being cut. \pause
            \item Hence we prove an analog of the proposition used in the exact recovery with $\gamma = \Theta(\log n)$. That is, if $\mathbf{\hat x}$ is an $\ell_2^2$ metric, then so is the similarly defined $\mathbf{\hat y}$.
        \end{itemize}

        \pause

        \begin{block}{Proposition}
            Fix an instance of {\sc Maximum Cut}, a cut $C$, and a feasible solution $\mathbf{\hat v}_i$'s to {\sc (VP-MaxCut)}. Let $\mathbf{\hat x}$ be the induced $\ell_2^2$ metric $\hat x_{ij} = \frac{1}{4} \lVert \mathbf v_i - \mathbf v_j \rVert^2$. For distinct $i, j \in V$, define
            $$ \hat y_{ij} = \left\{
                \begin{array}{ll}
                    \hat x_{ij}, & \text{if $i, j$ are on the same side of $C$}, \\
                    1 - \hat x_{ij}, & \text{if $i, j$ are on different sides of $C$}.
                \end{array}
            \right. $$
            Then $\mathbf {\hat y}$ is also an $\ell_2^2$ metric.
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{Improved Exact Recovery, Roadmap}
    
        \begin{proof}
            \begin{itemize}
                \item $\mathbf{\hat y}$ is a metric by the triangle inequality constraints of {\sc (VP-MaxCut)}. \pause
                \item To prove that $\mathbf{\hat y}$ is an $\ell_2^2$ metric, we need to exhibit an embedding of $V$ into $\RR^k$ for some $k$ such that $\hat y_{ij}$'s are squared Euclidean distances between points in $\RR^k$. \pause
                \item Define
                $$ \hat u_i = \left\{
                    \begin{array}{ll}
                        \hat v_i, & \text{if $i \in A$}, \\
                        -\hat v_i, & \text{if $i \in B$}.
                    \end{array}
                \right. $$
                \pause
                \item Then $\hat y_{ij} = \frac{1}{4} \lVert \hat u_i - \hat u_j \rVert^2$ for any distinct $i, j \in V$.
            \end{itemize}
        \end{proof}
    \end{frame}

    \begin{frame}
        \frametitle{Improved Exact Recovery, Roadmap}

        \begin{lemma}
            Fix an instance of {\sc Maximum Cut}, with $F^*$ the edges in the optimal cut, and $\mathbf v_i$'s the optimal solution to {\sc (VP-MaxCut)}. Then there exists a randomized algorithm that generates a random cut $(A, B)$ and a scaling parameter $\sigma > 0$ such that:
            \begin{enumerate}
                \item For every edge $e = ij \not \in F^*$,
                $$ \PP[\text{$e$ cut by $(A, B)$}] \geq \sigma \cdot \frac{\frac{1}{4} \lVert \mathbf v_i - \mathbf v_j \rVert^2}{\alpha}, $$
                where $\alpha = \Theta(\sqrt{\log n} \log \log n)$;
                \item For every edge $e = ij \in F^*$,
                $$ \PP[\text{$e$ not cut by $(A, B)$}] \leq \sigma \cdot \left(1 - \frac{1}{4} \left\lVert \mathbf v_i - \mathbf v_j \right\rVert\right); $$
                \item The rounding algorithm is deterministic iff.\ $\mathbf{\hat v}_i$'s are integral.
            \end{enumerate}
        \end{lemma}
    \end{frame}

    \begin{frame}
        \frametitle{Improved Exact Recovery, Roadmap}

        \begin{itemize}
            \item By $\mathbf{\hat v_i}$'s being {\bf integral}, we mean that there exist antipodal unit vectors $\mathbf w, -\mathbf w$ such that $\mathbf{\hat v_i} \in \{ \mathbf w, -\mathbf w \}$ for each $i \in V$. \pause
            \item As the lemma implies the main theorem in the exact recovery with $\gamma = \Theta(\log n)$, the above lemma implies the following theorem by an analagous argument:
        \end{itemize}

        \pause
    
        \begin{theorem}
            There is a constant $c > 0$ such that in every $(c \sqrt{\log n} \log \log n)$-perturbation-stable instance of {\sc Maximum Cut} with $n$ vertices, every optimal solution to {\sc (VP-MaxCut)} is integral.
        \end{theorem}
    \end{frame}

    \begin{frame}
        \frametitle{Can We Do Better?}

        \begin{theorem}
            There exist $O(\sqrt{\log n})$-perturbation-stable instances of {\sc Maximum Cut} for which the optimal solution to {\sc (VP-MaxCut)} is not integral.
        \end{theorem}

        \pause
    
        \begin{theorem}
            Assuming the Unique Game Conjecture, for every constant $\gamma \geq 1$, there is no polynomial-time algorithm for certifiable exact recovery in $\gamma$-perturbation-stable instances of {\sc Maximum Cut}.
        \end{theorem}

        \pause

        \begin{itemize}
            \item Both results are based upon a reduction from {\sc Sparsest Cut} to {\sc Maximum Cut}. See \emph{Biluâ€“Linial Stable Instances of Max Cut and Minimum Multiway} by Makarychev, Makarychev, and Vijayaraghavan (2013) and Homework \#4.
        \end{itemize}
    \end{frame}
\end{document}