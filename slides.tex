\documentclass{beamer}

\usepackage[utf8]{inputenc}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}

\DeclareMathOperator{\Uniform}{Uniform}

\setbeamercolor{block body alerted}{bg=alerted text.fg!10}
\setbeamercolor{block title alerted}{bg=alerted text.fg!20}
\setbeamercolor{block body}{bg=structure!10}
\setbeamercolor{block title}{bg=structure!20}
\setbeamercolor{block body example}{bg=green!10}
\setbeamercolor{block title example}{bg=green!20}
\setbeamertemplate{blocks}[rounded][shadow]

\title{Perturbation-Stable Maximum Cut}
\author{Yuchong Pan}
\institute{UBC Beyond Worst-Case Analysis Reading Group \\ (Based on Tim Roughgarden's Notes for Stanford CS264)}
%\date{\today}
\date{June 30, 2020}

\begin{document}
    \frame{\titlepage}

    \begin{frame}
        \frametitle{{\sc Maximum Cut}}
    
        \begin{block}{Problem ({\sc Maximum Cut})}
            \setlength{\leftmargini}{3.25em}
            \begin{itemize}
                \item[\bf Input:] An undirected graph $G = (V, E)$ with edge weights $w_e > 0$ for each $e \in E$.
                \item[\bf Goal:] A cut $(A, B)$ that maximizes the weight of the \emph{crossing} edges. 
            \end{itemize}
        \end{block}

        \pause

        \begin{itemize}
            \item {\sc Maximum Cut} is a type of $2$-clustering problem (e.g. weights measure dissimilarities).
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{{\sc Maximum Cut} Is $NP$-Hard}
    
        \begin{block}{Problem ({\sc Maximum Cut}, Decision Version)}
            \setlength{\leftmargini}{4em}
            \begin{itemize}
                \item[\bf Input:] An undirected graph $G = (V, E)$ with edge weights $w_e > 0$ for each $e \in E$, and a positive integer $W$.
                \item[\bf Output:] Yes iff.\ there is a set $S \subseteq V$ such that the weight of the \emph{crossing} edges is at least $W$.
            \end{itemize}
        \end{block}

        \pause

        \begin{block}{Problem ({\sc Partition}, Decision Version)}
            \setlength{\leftmargini}{4em}
            \begin{itemize}
                \item[\bf Input:] $(c_1, \ldots, c_n) \in \ZZ^n$.
                \item[\bf Output:] Yes iff.\ there is $I \subseteq [n]$ such that $\sum_{i \in I} c_i = \sum_{i \not \in I} c_i$.
            \end{itemize}
        \end{block}

        \pause

        \begin{block}{Proof Sketch ($\textsc{Partition} \leq_P \textsc{Maximum Cut}$)}
            \begin{itemize}
                \item $G = K_n$.
                \item $w_{ij} = c_i c_j$ for all $i, j \in V, i \neq j$.
                \item $W = \lceil \frac{1}{4} \sum c_i^2 \rceil$.
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        \frametitle{{\sc Maximum Cut} Is $NP$-Hard}
    
        \begin{itemize}
            \item {\sc Minimum Cut} is \emph{not} $NP$-hard and can be solved by the Maximum-Flow Minimum-Cut Theorem. \pause
            \item {\bf Question:} Can't we negate the edge weights, yielding a {\sc Minimum Cut} instance? \pause
            \item No! Polynomial-time algorithms solving {\sc Minimum Cut} require nonnegative edge weights.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Exact Recovery}
    
        \begin{itemize}
            \item {\bf Theme:} To recover the optimal solution in polynomial time in \emph{$\gamma$-perturbation-stable} instances, where $\gamma$ is as small as possible.
        \end{itemize}

        \pause

        \begin{definition}[$\gamma$-Perturbation-Stability]
            For $\gamma \geq 1$, an instance of {\sc Maximum Cut} is \emph{$\gamma$-perturbation-stable} if a cut $(A, B)$ is the \emph{unique} optimal solution to all \emph{$\gamma$-perturbations}, where each original edge weight $w_e$ is replaced with an edge weight $w_e' \in [\frac{1}{\gamma} w_e, w_e]$.
        \end{definition}
    \end{frame}

    \begin{frame}
        \frametitle{LP Relaxation, Take 1}
    
        \begin{itemize}
            \item {\bf Question:} Can we use an LP relaxation similar to the one for {\sc Minimum Cut}, i.e.
            \begin{alignat*}{4}
                && \max \qquad && \sum_{e \in E} w_e x_e \\
                && \text{s.t.} \qquad && x_e & \geq \left|d_u - d_v\right|, & \qquad \forall e = uv & \in E. \\
                && && x_e & \in [0, 1], & \qquad \forall e & \in E. \\
                && && d_v & \in [0, 1], & \qquad \forall v & \in V.
              \end{alignat*}
              \pause
              \vspace{-1em}
              \item No! $x_e = 1$ for each $e \in E$ is a feasible solution and maximizes the objective value. \pause
              \item {\bf Question:} What about $x_e \leq d_u - d_v$ and $x_e \leq d_v - d_u$? \pause
              \item This forces $x_e = 0$, instead of $x_e \leq |d_u - d_v|$.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{LP Relaxation, Take 2}
    
        \begin{itemize}
            \item Let $x_{ij} \in \{ 0, 1 \}$ denote whether or not $i, j$ are on different sides of the cut, for all distinct $i, j \in V$. We denote by $x_{ij}$ and $x_{ji}$ the same variable. \pause
            \item {\bf Intuition:} If $i, j$ are on different sides, and $i, k$ are also on different sides, then $j, k$ must be on the same sides. \pause
            \item For any distinct $i, j, k \in V$, at most two of $x_{ij}, x_{ik}, x_{jk}$ are $1$. \pause
            \begin{align*}
                x_{ij} + x_{ik} + x_{jk} \leq 2, && \forall i, j, k \in V \text{ distinct}.
            \end{align*}
            \pause
            \vspace{-1em}
            \item {\bf Intuition:} If $i, j$ are on the same side, and $i, k$ are on the same side, then $j, k$ are on the same side. \pause
            \item For any distinct $i, j, k \in V$, $x_{ij} = x_{ik} = 0$ implies $x_{jk} = 0$. \pause
            \begin{align*}
                x_{jk} \leq x_{ij} + x_{ik}, && \forall i, j, k \in V \text{ distinct}.
            \end{align*}
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{LP Relaxation, Take 2}
    
        \begin{itemize}
            \item Hence we obtain the LP relaxation {\sc (LP-MaxCut)}:
            \begin{alignat*}{4}
                && \max \qquad && \sum_{(i, j) \in E} w_{ij} x_{ij} \\
                && \text{s.t.} \qquad && x_e & \geq \left|d_u - d_v\right|, & \quad \forall e = uv & \in E. \\
                && && x_{ij} + x_{ik} + x_{jk} & \leq 2, & \quad \forall i, j, k & \in V \text{ distinct}. \\
                && && x_{jk} & \leq x_{ij} + x_{ik}, & \quad \forall i, j, k & \in V \text{ distinct}. \\
                && && x_{ij} & \in [0, 1], & \quad \forall i, j & \in V \text{ distinct}.
              \end{alignat*}
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Main Theorem}

        \begin{theorem}
            There is a constant $c > 0$ such that in every $(c \log n)$-perturbation-stable instance of {\sc Maximum Cut} with $n$ vertices, {\sc (LP-MaxCut)} solves to integers.
        \end{theorem}

        \pause

        \begin{itemize}
            \item Recall the proofs of exact recovery by LP in $1$-perturbation-stable {\sc Minimum $s$-$t$ Cut} instances and in $4$-perturbation-stable {\sc Minimum Multiway Cut} instances. \pause
            \item In each of the two proofs we design a {\bf randomized rounding algorithm} that outputs a (random) cut such that the probablity of an edge being cut is approximately the same as the value of the corresponding decision variable. \pause
            \item {\sc Minimum $s$-$t$ Cut}: $A = \{ v \in V : \hat d_v \leq r \}$ and $B = V \setminus A$, where $r \sim \Uniform(0, 1)$. \pause
            \item {\sc Minimum Multiway Cut}: For each iteration, a group and a threshold are chosen uniformly randomly.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Main Theorem}
    
        \begin{fact}
            LP algorithms (e.g.\ the ellipsoid method) always return an extreme point of the feasible region.
        \end{fact}

        \pause

        \begin{itemize}
            \item Proof omitted here. For the ellipsoid method see e.g.\ CPSC 536S Submodular Optimization. \pause
            \includegraphics[width=0.9\textwidth]{ellipsoid.png}
            \pause
            \item Since all of the extreme points of the feasible region are integral and correspond to a cut, then LP algorithms always solve {\sc (LP-MaxCut)} to an integral optimal solution.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Main Theorem}

        \begin{itemize}
            \item A randomized rounding algorithm implies the exact recovery theorem since: \pause
            \begin{enumerate}
                \item The optimal fractional solution $\mathbf{\hat x}$ can only be better than the optimal integral solution $C^*$; \pause
                \item The randomized rounding algorithm gives a distribution over $s$-$t$ cuts that is as good, on average, as $C^*$; \pause
                \item Hence the distribution must be a point mass on $C^*$.
            \end{enumerate}
            \pause
            \item Formally, we define $\Delta(C)$ to be the total cost of $C$ that exceeds that of $C^*$ and $\Delta(\mathbf{\hat x})$ to be total cost of $C^*$ that exceeds the objective function value of $\mathbf{\hat x}$. \pause
            \item We show that $\EE[\Delta(C)] \leq 0$ by the probablity properties of the cut generated by the randomized rounding algorithm. \pause
            \item Since $\Delta(C) \geq 0$ and since the equality holds iff.\ $C$ is an optimal cut, it follows that the randomized rounding algorithm outputs an optimal cut w.p.1.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Randomized Rounding Algorithm}
    
        \begin{lemma}
            Fix an instance of the {\sc Maximum Cut} problem, with $F^*$ the edges in the optimal cut, and $\mathbf{\hat x}$ the optimal solution to {\sc (LP-MaxCut)}. Then there exists a randomized algorithm that generates a random cut $(A, B)$ and a scaling parameter $\sigma > 0$ such that:
            \begin{enumerate}
                \item For every edge $e = ij \not \in F^*$,
                $$ \PP[\text{$e$ cut by $(A, B)$}] \geq \sigma \cdot \frac{\hat x_{ij}}{\alpha}, $$
                where $\alpha = \Theta(\log n)$;
                \item For every edge $e = ij \in F^*$,
                $$ \PP[\text{$e$ not cut by $(A, B)$}] \leq \sigma \cdot (1 - \hat x_{ij}); $$
                \item The rounding algorithm is determinisitic iff.\ $\hat x$ is integral.
            \end{enumerate}
        \end{lemma}
    \end{frame}

    \begin{frame}
        \frametitle{Randomized Rounding Algorithm, Roadmap}

        \begin{itemize}
            \item {\bf Exercise:} Show that this lemma implies the main theorem (outlined above, Homework \#4).
        \end{itemize}

        \pause

        \begin{block}{Proposition}
            Fix an instance of {\sc Maximum Cut}, a cut $C$, and a feasible solution $\mathbf{\hat x}$ to {\sc (LP-MaxCut)}. For distinct $i, j \in V$, define
            $$ \hat y_{ij} = \left\{
                \begin{array}{ll}
                    \hat x_{ij}, & \text{if $i, j$ are on the same side of $C$}, \\
                    1 - \hat x_{ij}, & \text{if $i, j$ are on different sides of $C$}.
                \end{array}
            \right. $$
            Then $\mathbf {\hat y}$ satisfies the triangle inequality:
            $$ \hat y_{jk} \leq \hat y_{ij} + \hat y_{ik} $$
            for every $i, j, k \in V$.
        \end{block}

        \pause

        \begin{itemize}
            \item That is, $\mathbf{\hat x}, \mathbf{\hat y}$ are both \emph{semi-metrics} (metrics except that distinct points may have zero distances).
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Randomized Rounding Algorithm, Roadmap}
    
        \begin{theorem}[Bourgain's Theorem]
            For every $n$-point \emph{semi-metric} space $(X, d)$, there exists a randomized algorithm that generates a random partition $(A, B)$ of $X$ and a scaling parameter $\sigma > 0$ such that, for all distinct $i, j \in X$,
            $$ \PP[\text{$i, j$ on different sides of $(A, B)$}] \in \sigma \cdot \left[\frac{d(i, j)}{\alpha}, d(i, j)\right], $$
            where $\alpha = \Theta(\log n)$.
        \end{theorem}

        \pause

        \begin{itemize}
            \item That is, every $n$-point metric space admits a randomized partitioning algorithm so that the sepration probabilities between pairs of points are \emph{proportional} to the distances, up to a $\Theta(\log n)$ factor. \pause
            \item The $\Theta(\log n)$ approximation factor is the best possible for \emph{arbitrary} semi-metric spaces.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Randomized Rounding Algorithm, Roadmap}
    
        \begin{proof}[Proof (Proposition \& Bourgain's Theorem $\implies$ Lemma)]
            \renewcommand{\qedsymbol}{}
            \begin{itemize}
                \item Fix an instance of {\sc Maximum Cut}. Let $C^*$ denote an optimal cut, cutting the edges $F^*$. \pause
                \item Let $\mathbf{\hat x}$ be an optimal solution to {\sc (LP-MaxCut)}. Define $\mathbf{\hat y}$ as in Proposition (with $C^*$ being the cut). \pause
                \item By Proposition, $\mathbf{\hat y}$ is a semi-metric. \pause
                \item By Bourgain's Theorem, there is a randomized algorithm that outputs a partition $(A, B)$ and $\sigma > 0$ such that
                $$ \PP[\text{$i, j$ on different sides of $(A, B)$}] = \sigma \cdot \left[\frac{\hat y_{ij}}{\alpha}, \hat y_{ij}\right], $$
                where $\alpha = \Theta(\log n)$.
            \end{itemize}
        \end{proof}
    \end{frame}

    \begin{frame}
        \frametitle{Randomized Rounding Algorithm, Roadmap}
    
        \begin{proof}[Proof (Proposition \& Bourgain's Theorem $\implies$ Lemma)]
            \begin{itemize}
                \item By the definition of $\mathbf{\hat y}$,
                \begin{enumerate}
                    \item If $i, j$ are on the same side of $C^*$, then
                    $$ \PP[\text{$i, j$ on different sides of $(A, B)$}] \in \sigma \cdot \left[\frac{\hat x_{ij}}{\alpha}, \hat x_{ij}\right]. $$
                    \item If $i, j$ are on different sides of $C^*$, then
                    $$ \PP[\text{$i, j$ on different sides of $(A, B)$}] \in \sigma \cdot \left[\frac{1 - \hat x_{ij}}{\alpha}, 1 - \hat x_{ij}\right]. $$
                \end{enumerate}
                \pause
                \item Lemma follows.
            \end{itemize}
        \end{proof}

        \pause

        \begin{itemize}
            \item {\bf Exercise:} Prove Proposition and Bourgain's Theorem (Homework \#4). For Bourgain's Theorem see e.g.\ CPSC 531F Tools for Modern Algorithm Analysis.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{Metric Embedding}
    \end{frame}
\end{document}